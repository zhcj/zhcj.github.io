<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv=Content-Type content="text/html; charset=utf-8">
<meta http-equiv=pragma content=no-cache>
<link href="css/bootstrap.min.css" rel="stylesheet">
<link rel="shortcut icon" href="favicon.ico">
<link rel="bookmark" href="favicon.ico">
<title>Ollama+DeepSeek+Open-WebUI/Chatbox/Cherry-Studio安装</title>
</head>
<body>
<div class="container">
<p><a href="index.html">返回首页</a></p>
<h2 align=center>Ollama+DeepSeek+Open-WebUI/Chatbox/Cherry-Studio安装</h2>
<p>注：Mac mini/Windows11/AlmaLinux9测试通过。</p>
<p><b>一、安装Ollama，官网：<a href="https://ollama.com" target="_blank">https://ollama.com</a></b></p>
<p>1、各平台安装Ollama，官网下载安装包或脚本安装</p>
<p>2、Linux命令行手动下载二进制包</p>
<p>2.1、下载Ollama（根据你的系统架构下载相应版本），解压</p>
<pre>wget https://ollama.com/download/ollama-linux-amd64.tgz
sudo tar xvf ollama-inux-amd64.tgz -C /usr</pre>
<p>2.2、添加ollama用户及组，将当前用户加入ollama组</p>
<pre>sudo useradd -r -s /bin/false -U -m -d /usr/share/ollama ollama
sudo usermod -a -G ollama $(whoami)</pre>
<p>2.3、创建启动脚本</p>
<pre>sudo cat > /lib/systemd/system/ollama.service << "EOF"
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
#以下两行将11434端口打开，为了安全，需要用防火墙限制IP等方式控制权限；如用nginx转发模式（详见附1）或安装open-webui客户端，不用打开11434端口，将下面两行注释即可
Environment="OLLAMA_HOST=0.0.0.0"
Environment="OLLAMA_ORIGINS=*"
ExecStart=/usr/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="PATH=$PATH"

[Install]
WantedBy=default.target
EOF
</pre>
<p>2.4、启动并添加开机启动</p>
<pre>sudo systemctl start ollama
sudo systemctl enable ollama
</pre>
<p>3、Linux平台源码编译安装</p>
<pre>
#go设置代理加速下载
go env -w GO111MODULE=on
go env -w  GOPROXY=https://goproxy.cn,direct
git clone https://github.com/ollama/ollama
cd ollama
cmake -B build
cmake --build build
go run . serve
</pre>
<p>注：MacOS打开Ollama的11434端口方式n</p>
<pre>
launchctl setenv OLLAMA_HOST "0.0.0.0"
launchctl setenv OLLAMA_ORIGINS "*"
</pre>
<p><b>二、Ollama安装DeepSeek</b></p>
<p>所有平台均可以打开终端，运行以下命令安装</p>
<pre>ollama run deepseek-r1:8b</pre>
<p>注：<p>
<p>#<a href="https://ollama.com/models" target="_blank">https://ollama.com/models</a>可以查看可以安装的各种大模型；<p>
<p>#8b中的b是billion（10亿），也就是80亿个参数。<p>
<p><b>三、安装客户端</b></p>
<p>说明：</p>
<p>#以下软件均在快速开发迭代中（甚至一天多个版本发布），请随时关注其官方网站；</p>
<p>#所有客户端均需要调用Ollama的11434端口，特别要注意服务端的安全；</p>
<p>#提供web端的，如Open-WebUI，服务端可以关闭11434端口和Open-WebUI的8080端口，将8080端口用Nginx转发至其他端口（详见附2），更安全。</p>
<p>1、Open-WebUI，服务端安装，适合多用户，浏览器打开网页即可使用</p>
<p>各平台安装Python-3.11（必须是此版本），安装好运行行以下命令</p>
<pre>
pip install open-webui
open-webui serve
</pre>
<p>注：<p>
<p>#服务器端安装好，第一次网页打开很慢，打开后需要设置用户名、邮箱、密码来中注册管理员账号；<p>
<p>#管理员登录后，建议进行以下设置：设置外部连接中的Ollama API，并设置为默认模型；关闭OpenAI API加快启动速度；将Ollama大模型由Private设置为Public；开放账号注册；<p>
<p>#用户打开http://服务端IP:8080，用邮箱和密码注册，登录使用。<p>
<p>2、Chatbox，支持所有平台，有移动端<p>
<p>2.1、Chatbox安装包，下载地址：<a href="https://chatboxai.app" target="_blank">https://chatboxai.app</a></p>
<p>2.2、Chatbox源码编译，支持所有平台</p>
<p>安装nodejs-20（必须是此版本），运行行以下命令</p>
<pre>
git clone https://github.com/Bin-Huang/chatbox
cd chatbox
#将package.json中的tailwindcss版本由4.0.0改为3.4，手动修改可使用以下命令
sed -i "" "s/\"tailwindcss\": \"\^4\.0\.0\"/\"tailwindcss\": \"\^3\.4\"/g" package.json
npm install
npm run dev
</pre>
<p>3、Cherry Studio，除移动端外，支持其他所有平台<p>
<p>3.1、Cherry Studio安装包，下载地址：<a href="https://cherry-ai.com/download" target="_blank">https://cherry-ai.com/download</a></p>
<p>3.2、Cherry Studio源码编译，支持所有平台</p>
<p>安装yarn最新版本，运行行以下命令</p>
<pre>
#yarn安装
corepack enable
yarn set version stable
git clone https://github.com/CherryHQ/cherry-studio
cd cherry-studio
yarn
yarn run dev
</pre>
<p><b>附1：Nginx转发Ollama默认11434端口到其他端口（如11435），方便控制权限</b><p>
<pre>
server {
    listen      11435;
    server_name 127.0.0.1;

    allow   127.0.0.1;
    allow   10.14.0.0/16;
    allow   192.168.1.0/24;
    deny    all;

    access_log	../log/ollama_access.log main;
    error_log	../log/ollama_error.log error;

    location / {
        proxy_pass  http://127.0.0.1:11434;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
</pre>
<p><b>附2：Nginx转发Open-WebUI默认8080端口到其他端口（如80），方便控制权限</b><p>
<pre>
server {
    listen      80;
    server_name 127.0.0.1;

    allow   127.0.0.1;
    allow   10.14.0.0/16;
    allow   192.168.1.0/24;
    deny    all;

    access_log	../log/open-webui_access.log main;
    error_log	../log/open-webui_error.log error;

    location / {
        proxy_pass  http://127.0.0.1:8080;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    location /ws/ {
        proxy_pass http://127.0.0.1:8080/ws/;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "Upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
</pre>
<p><a href="index.html">返回首页</a></p>
<p align="center">&copy; 2016-2025 清风的个人笔记</p>
</div>
</body>
</html>
