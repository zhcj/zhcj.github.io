<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv=Content-Type content="text/html; charset=utf-8">
<meta http-equiv=pragma content=no-cache>
<link href="css/bootstrap.min.css" rel="stylesheet">
<link rel="shortcut icon" href="favicon.ico">
<link rel="bookmark" href="favicon.ico">
<title>Ollama+DeepSeek+Open-WebUI/Chatbox/Cherry-Studio安装</title>
</head>
<body>
<div class="container">
<p><a href="index.html">返回首页</a></p>
<h2 align=center>Ollama+DeepSeek+Open-WebUI/Chatbox/Cherry-Studio安装</h2>
<p>注：Mac mini/Windows11/AlmaLinux9测试通过。</p>
<p><b>一、安装Ollama</b></p>
<p>1、官网下载安装包，下载地址：<a href="https://ollama.com" target="_blank">https://ollama.com</a></p>
<p>2、Linux命令行手动下载二进制包安装</p>
<p>2.1、下载Ollama（根据你的系统架构下载相应版本），解压</p>
<pre>wget https://ollama.com/download/ollama-linux-amd64.tgz
sudo tar xvf ollama-inux-amd64.tgz -C /usr</pre>
<p>2.2、添加ollama用户及组，将当前用户加入ollama组</p>
<pre>sudo useradd -r -s /bin/false -U -m -d /usr/share/ollama ollama
sudo usermod -a -G ollama $(whoami)</pre>
<p>2.3、创建启动脚本</p>
<pre>sudo bash -c "cat > /lib/systemd/system/ollama.service" << EOF
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
#以下两行将11434端口打开，为了安全，需要用防火墙限制IP等方式控制权限；如用nginx转发模式（详见附1）或安装open-webui客户端，不用打开11434端口，将下面两行注释即可
Environment="OLLAMA_HOST=0.0.0.0"
Environment="OLLAMA_ORIGINS=*"
ExecStart=/usr/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="PATH=$PATH"

[Install]
WantedBy=default.target
EOF
</pre>
<p>2.4、启动并添加开机启动</p>
<pre>sudo systemctl start ollama
sudo systemctl enable ollama
</pre>
<p>3、Linux/MacOS平台源码编译运行</p>
<p>需要安装gcc、cmake、go等编译工具</p>
<pre>
#MacOS安装编译工具
brew install go cmake
#go设置代理加速下载
go env -w GO111MODULE=on
go env -w  GOPROXY=https://goproxy.cn,direct
git clone https://github.com/ollama/ollama
cd ollama
cmake -B build
cmake --build build
go run . serve
#编译成可执行文件运行
go build -o build/ollama main.go
sudo cp build/ollama /usr/local/bin/
sudo cp -r build/lib/ollama /usr/local/lib/
ollama serve
</pre>
<p>注：MacOS打开Ollama的11434端口命令</p>
<pre>
launchctl setenv OLLAMA_HOST "0.0.0.0"
launchctl setenv OLLAMA_ORIGINS "*"
</pre>
<p><b>二、Ollama安装DeepSeek</b></p>
<p>所有平台均可以打开终端，运行以下命令安装</p>
<pre>ollama run deepseek-r1:8b</pre>
<p>注：<p>
<p>#<a href="https://ollama.com/models" target="_blank">https://ollama.com/models</a>可以查看可以安装的各种模型；<p>
<p>#8b中的b是billion（10亿），也就是80亿个参数。<p>
<p><b>三、安装客户端</b></p>
<p>说明：</p>
<p>#以下软件均在快速开发迭代中（甚至一天多个版本发布），请随时关注其官方网站；</p>
<p>#所有客户端均需要调用Ollama的11434端口，特别要注意服务端的安全；</p>
<p>1、Open-WebUI，服务端安装，适合多用户，浏览器打开网页即可使用</p>
<p>官方要求安装Python-3.11（经测试，Python-3.12也可以），然后运行以下命令（AlmaLinux9由于sqlite版本为3.34，低于3.35，能pip安装，但无法运行，暂未解决）</p>
<pre>
pip install open-webui
open-webui serve
#升级
pip install --upgrade open-webui
</pre>
<p>注：<p>
<p>#如没有安装ffmpeg，终端会提示，建议安装</p>
<p>#第一次网页打开很慢（默认去连接OpenAI的API），打开后需要设置用户名、邮箱、密码来中注册管理员账号；<p>
<p>#管理员登录后，建议进行以下设置：设置外部连接中的Ollama API，并设置为默认模型；关闭OpenAI API，加快启动速度；将Ollama模型由Private设置为Public；开放账号注册；<p>
<p>#用户打开http://服务端IP:8080，用邮箱和密码注册，登录使用；<p>
<p>#服务端可以关闭11434端口和8080端口，再将8080端口用Nginx转发至其他端口（详见附2），方便控制权限。</p>
<p>2、LobeChat，和Ollama同一服务器或单独服务器安装，适合多用户，浏览器打开网页即可使用</p>
<p>安装pnpm，然后运行以下命令</p>
<pre>
git clone https://github.com/lobehub/lobe-chat
cd lobe-chat
pnpm install
pnpm run dev
</pre>
<p>#和Open-WebUI相比，功能更强，因此运行需要消耗更多系统资源；</p>
<p>#Ollama服务端需要打开11434端口，和Open-WebUI不同的是，只是数据转发，不是代理；</p>
<p>#用户打开http://LobeChat服务端IP:3010，自行设置地址（如果是和Ollama同一服务器，默认留空，否则为http://Ollama服务端IP:11434，默认留空）及模型（如：deepseek-r1:8b，新增，多个逗号隔开）后使用。<p>
<p>3、Chatbox，支持所有平台，有移动端<p>
<p>3.1、Chatbox安装包，下载地址：<a href="https://chatboxai.app" target="_blank">https://chatboxai.app</a></p>
<p>3.2、Chatbox源码编译，支持所有平台</p>
<p>安装nodejs-20（必须是此版本），运行行以下命令</p>
<pre>
git clone https://github.com/Bin-Huang/chatbox
cd chatbox
npm install
npm run dev
</pre>
<p>4、Cherry Studio，除移动端外，支持其他所有平台<p>
<p>4.1、Cherry Studio安装包，下载地址：<a href="https://cherry-ai.com/download" target="_blank">https://cherry-ai.com/download</a></p>
<p>4.2、Cherry Studio源码编译，支持所有平台</p>
<p>安装yarn最新版本，运行以下命令</p>
<pre>
#安装yarn
corepack enable
yarn set version stable
git clone https://github.com/CherryHQ/cherry-studio
cd cherry-studio
yarn
yarn dev
</pre>
<p><b>附1：Nginx转发Ollama默认11434端口到其他端口（如11435），方便控制权限</b><p>
<pre>
server {
    listen      11435;
    server_name 127.0.0.1;

    allow   127.0.0.1;
    allow   192.168.1.0/24;
    deny    all;

    access_log	../log/ollama_access.log main;
    error_log	../log/ollama_error.log error;

    location / {
        proxy_pass  http://127.0.0.1:11434;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
</pre>
<p><b>附2：Nginx转发Open-WebUI默认8080端口到其他端口（如80），方便控制权限</b><p>
<pre>
server {
    listen      80;
    server_name 127.0.0.1;

    allow   127.0.0.1;
    allow   192.168.1.0/24;
    deny    all;

    access_log	../log/open-webui_access.log main;
    error_log	../log/open-webui_error.log error;

    location / {
        proxy_pass  http://127.0.0.1:8080;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    location /ws/ {
        proxy_pass http://127.0.0.1:8080/ws/;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "Upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
</pre>
<p><a href="index.html">返回首页</a></p>
<p align="center">&copy; 2016-2025 清风的个人笔记</p>
</div>
</body>
</html>
